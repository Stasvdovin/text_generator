[![Tests](https://github.com/Stasvdovin/text_generator/actions/workflows/python-app.yml/badge.svg)](https://github.com/Stasvdovin/text_generator/actions/workflows/python-app.yml)
Project "Text Generatin"
https://stasvdovin-text-generator-main-application-4v6b6n.streamlit.app/

## Content
0. [Team](.README.md#Team)
1. [Project description](README.md#Project-description)
2. [Direct Use](README.md#What-problem-are-we-solving)
3. [Downstream Use](README.md#Short-info-about-initial-data)
4. [Testing Data, Factors and Metrics](README.md#Stages-of-the-project)
5. [Results](README.md#Results)

### Team
- Vdovin Stanislav (Stasvdovin)
- Dobrovinskaia Polina (PolinaDobrovinskaia)


:arrow_up:[to Content](README.md#Content)

### Project description
Openai-gpt is a transformer-based language model created and released by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long range dependencies.

:arrow_up:[to Content](README.md#Content)


### Direct Use
This model can be used for language modeling tasks.


### Downstream Use
Potential downstream uses of this model include tasks that leverage language models. In the associated paper, the model developers discuss evaluations of the model for tasks including natural language inference (NLI), question answering, semantic similarity, and text classification.

:arrow_up:[to Content](README.md#Content)


### Testing Data, Factors and Metrics
The model developers report that the model was evaluated on the following tasks and datasets using the listed metrics:
•	Task: Textual Entailment
•	Datasets: SNLI, MNLI Matched, MNLI Mismatched, SciTail, QNLI, RTE
•	Metrics: Accuracy
•	Task: Semantic Similarity
•	Datasets: STS-B, QQP, MRPC
•	Metrics: Accuracy
•	Task: Reading Comprehension
•	Datasets: RACE
•	Metrics: Accuracy
•	Task: Commonsense Reasoning
•	Datasets: ROCStories, COPA
•	Metrics: Accuracy
•	Task: Sentiment Analysis
•	Datasets: SST-2
•	Metrics: Accuracy
•	Task: Linguistic Acceptability
•	Datasets: CoLA
•	Metrics: Accuracy
•	Task: Multi Task Benchmark
•	Datasets: GLUE
•	Metrics: Accuracy


:arrow_up:[to Content](README.md#Content)


### Results:
The model achieves the following results without any fine-tuning (zero-shot)


